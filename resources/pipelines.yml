# Lakeflow Declarative Pipeline Configuration
resources:
  pipelines:
    customer_complaints_pipeline:
      name: "customer_complaints_r_migration_pipeline"
      catalog: ${var.catalog}
      schema: ${var.schema}
      serverless: ${var.pipeline_serverless}
      
      # All-purpose cluster configuration for development (when serverless: false)
      clusters:
        - node_type_id: "Standard_D8s_v3"  # Medium-sized instances for development work
          driver_node_type_id: "Standard_D8s_v3"
          num_workers: 2
          spark_conf:
            "spark.databricks.delta.preview.enabled": "true"
            "spark.sql.adaptive.enabled": "true"
            "spark.sql.adaptive.coalescePartitions.enabled": "true"
          custom_tags: ${var.environment_tags}
      
      # Explicitly include each transformation file
      libraries:
        - file:
            path: ../src/pipelines/transformations/customer_complaints_bronze.sql
        - file:
            path: ../src/pipelines/transformations/customer_complaints_silver.sql
        - file:
            path: ../src/pipelines/transformations/customer_complaints_gold.sql
      
      # Configuration parameters
      configuration:
        volume_path: ${var.volume_path}
        
      # Environment settings for dependencies
      environment:
        dependencies:
          # Include any Python dependencies as YAML list if needed
          - scikit-learn

      # Notifications for pipeline failures
      notifications:
        - email_recipients:
            - daniel.zoccali@databricks.com
          alerts:
            - on-update-failure
            - on-update-fatal-failure
            - on-flow-failure


